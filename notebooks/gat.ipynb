{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_gat(config):\n",
    "    global BEST_VAL_ACC, BEST_VAL_LOSS\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU, I hope so!\n",
    "\n",
    "    # Step 1: load the graph data\n",
    "    node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
    "\n",
    "    # Step 2: prepare the model\n",
    "    gat = GAT(\n",
    "        num_of_layers=config['num_of_layers'],\n",
    "        num_heads_per_layer=config['num_heads_per_layer'],\n",
    "        num_features_per_layer=config['num_features_per_layer'],\n",
    "        add_skip_connection=config['add_skip_connection'],\n",
    "        bias=config['bias'],\n",
    "        dropout=config['dropout'],\n",
    "        log_attention_weights=False  # no need to store attentions, used only in playground.py while visualizing\n",
    "    ).to(device)\n",
    "\n",
    "    # Step 3: Prepare other training related utilities (loss & optimizer and decorator function)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    # THIS IS THE CORE OF THE TRAINING (we'll define it in a minute)\n",
    "    # The decorator function makes things cleaner since there is a lot of redundancy between the train and val loops\n",
    "    main_loop = get_main_loop(\n",
    "        config,\n",
    "        gat,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        node_features,\n",
    "        node_labels,\n",
    "        edge_index,\n",
    "        train_indices,\n",
    "        val_indices,\n",
    "        test_indices,\n",
    "        config['patience_period'],\n",
    "        time.time())\n",
    "\n",
    "    BEST_VAL_ACC, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]  # reset vars used for early stopping\n",
    "\n",
    "    # Step 4: Start the training procedure\n",
    "    for epoch in range(config['num_of_epochs']):\n",
    "        # Training loop\n",
    "        main_loop(phase=LoopPhase.TRAIN, epoch=epoch)\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                main_loop(phase=LoopPhase.VAL, epoch=epoch)\n",
    "            except Exception as e:  # \"patience has run out\" exception :O\n",
    "                print(str(e))\n",
    "                break  # break out from the training loop\n",
    "\n",
    "    # Step 5: Potentially test your model\n",
    "    # Don't overfit to the test dataset - only when you've fine-tuned your model on the validation dataset should you\n",
    "    # report your final loss and accuracy on the test dataset. Friends don't let friends overfit to the test data. <3\n",
    "    if config['should_test']:\n",
    "        test_acc = main_loop(phase=LoopPhase.TEST)\n",
    "        config['test_acc'] = test_acc\n",
    "        print(f'Test accuracy = {test_acc}')\n",
    "    else:\n",
    "        config['test_acc'] = -1\n",
    "\n",
    "    # Save the latest GAT in the binaries directory\n",
    "    torch.save(get_training_state(config, gat), os.path.join(BINARIES_PATH, get_available_binary_name()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
